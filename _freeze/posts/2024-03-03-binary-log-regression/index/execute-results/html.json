{
  "hash": "1ffba9116a93d0b7e6f7b103b7922ca4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Binary Logistic Regression\"\ndescription: \"Using binary logistic regression to differentiate plant species.\"\nauthor:\n  - name: Madison Calbert\n    url: https://madicalbert.github.io/\n    affiliation: ESM 244 Advanced Data Analysis - Master of Environmental Science & Management Program @ The Bren School (UCSB)\n    affiliation-url: https://bren.ucsb.edu/masters-programs/master-environmental-science-and-management \ndate: 03-03-2024\ncategories: [Data Science] # self-defined categories\ncitation: \n  url: https://madicalbert.github.io/posts/2024-03-03-binary-log-regression/ \nimage: preview-image.png\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\nformat: \n  html: \n    code-fold: true\n    toc: true \n    embed-resources: true\neditor: visual\nexecute: \n  echo: true\n  message: false\n  warning: false\n---\n\n\n\n![Serenoa repens is a flowering, perennial, wetland shrub that is commonly found in wet to dry flatwoods and hammocks throughout the state of Florida.](images/s_repens.jpg)\n\n## Overview\n\nSaw Palmetto (*Serenoa repens*) and Scrub Palmetto (*Sabal etonia*) are both species of palms native to Florida. In this report, I use binary logistic regression to test the feasibility of using different plant characteristics to differentiate between the two species. The different plant variables include plant height (height), canopy length (length), canopy width (width), and number of green leaves (green_lvs).\n\n## Data Source\n\nThe data for this analysis was sourced from Environmental Data Initiative Data Portal. This data package is comprised of three datasets all pertaining to two dominant palmetto species, *Serenoa repens* and *Sabal etonia*, at Archbold Biological Station in south-central Florida.\n\nData source: Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5\n\n## Pseudocode\n\n-   Load libraries, load data, clean/tidy the data, select desired variables (species, height, length, width, green_lvs)\n\n-   Data visualization to hypothesize which variables differ between species.\n\n-   Build the two models: Model 1 determines species based on height, length, width, and number of leaves; & Model 2 determines species based on height, width, and number of leaves (excluding length).\n\n-   Run the binary logistic regression on both models, using generalized linear model.\n\n-   Split the data into testing group and training group.\n\n-   Initialize workflow\n\n-   Apply workflow to folded training data set for both models.\n\n-   Train the whole data set with the best model to determine which variables best predict species.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Load libraries\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tidymodels)\n\nlibrary(cowplot)\nlibrary(kableExtra)\nlibrary(broom)\n\n\n### Load in the data\n\np_df <- read_csv(here('posts', '2024-03-03-binary-log-regression','data', 'palmetto.csv'))\n\n\n### Tidy the data and make species a factor\n\np_clean <- p_df %>% \n  select(species, height, length, width, green_lvs) %>% \n  mutate(species = as_factor(species)) %>% \n  drop_na()\n```\n:::\n\n\n\n## Methods\n\n### Data Visualization\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlvs_bp <- ggplot(p_clean, aes(x = as_factor(species), y = green_lvs)) + \n  geom_boxplot(fill = \"lightgreen\") + \n  labs(x = \" \", \n       y = \"Number of Leaves\") +\n  theme_minimal()+\n  scale_x_discrete(labels = c(\"S. repens\", \"S. etonia\"))\n\nheight_bp <- ggplot(p_clean, aes(x = as_factor(species), y = height)) + \n  geom_boxplot(fill = \"lightgreen\")+ \n  labs(x = \" \", \n       y = \"Height (cm)\") +\n  theme_minimal()+\n  scale_x_discrete(labels = c(\"S. repens\", \"S. etonia\"))\n\nlength_bp <- ggplot(p_clean, aes(x = as_factor(species), y = length)) + \n  geom_boxplot(fill = \"lightgreen\")+ \n  labs(x = \" \", \n       y = \"Canopy Length (cm)\") +\n  theme_minimal()+\n  scale_x_discrete(labels = c(\"S. repens\", \"S. etonia\"))\n\nwidth_bp <- ggplot(p_clean, aes(x = as_factor(species), y = width)) + \n  geom_boxplot(fill = \"lightgreen\")+ \n  labs(x = \" \", \n       y = \"Canopy Width (cm)\") +\n  theme_minimal()+\n  scale_x_discrete(labels = c(\"S. repens\", \"S. etonia\"))\n\n### put it all together now \n\ncombined_plot <- plot_grid(\n  lvs_bp, height_bp,\n  length_bp, width_bp,\n  ncol = 2  \n)\n\nprint(combined_plot)\n```\n\n::: {.cell-output-display}\n![Figure 1: Saw Palmetto (*Serenoa repens*) and Scrub Palmetto (*Sabal etonia*) differ by the number of leaves and are fairly similar based on height, canopy width, and canopy length. Number of leaves appears to be the best predictor variable to differentiate these two palmetto species.](index_files/figure-html/data viz-1.png){width=672}\n:::\n:::\n\n\n\n### Define the models\n\n-   Model 1: Species as a function of height, canopy length, canopy width, and number of leaves\n\n-   Model 2: Species as a function of height, canopy width, and number of leaves\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### sps. 1 = s. repens\n### sps. 2 = s. etonia\n\nf1 <- species ~ height + length + width + green_lvs \nf2 <- species ~ height + width + green_lvs\n```\n:::\n\n\n\n### Crossfold Validation\n\nModel 1 has an accuracy of 92%, this is the percent classified correctly in the testing group. In addition, the ROC is 97% which is good and explains that we do not have a lot of false positives. We are close to a perfect classifier.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_clean %>%\n  group_by(species) %>%\n  summarize(n = n()) %>%\n  ungroup() %>%\n  mutate(prop = n / sum(n))\n\nset.seed(10101)\np_folds <- vfold_cv(p_clean, v = 10, repeats = 10)\n\n\nblr_mdl <- logistic_reg() %>%\n  set_engine('glm') ### this is the default - we could try engines from other packages or functions\n\n\nblr_wf <- workflow() %>%   ### initialize workflow\n  add_model(blr_mdl) %>%\n  add_formula(formula = f1)\n\n\nblr_fit_folds <- blr_wf %>%\n  fit_resamples(p_folds)\n\n\n### Average the predictive performance of the ten models:\n# collect_metrics(blr_fit_folds)\n```\n:::\n\n\n\nModel 2 has an accuracy of 89%, so this model is not as accurate of a predictor compares to Model 1. In addition the ROC is 96% which is still a good value, but not as strong as Model 1.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblr2_wf <- workflow() %>%   ### initialize workflow\n  add_model(blr_mdl) %>%\n  add_formula(formula = f2)\n\n\nblr2_fit_folds <- blr2_wf %>%\n  fit_resamples(p_folds)\n\n\n### Average the predictive performance of the ten models:\n# collect_metrics(blr2_fit_folds)\n```\n:::\n\n\n\n### Check AIC\n\nModel 1 has a lower AIC compared to Model 2, so Model 1 is a better predictor of species.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblr1_fit <- blr_mdl %>%\n  fit(formula = f1, data = p_clean)\n\nblr2_fit <- blr_mdl %>%\n  fit(formula = f2, data = p_clean)\n\n# blr1_fit\n### AIC of 5195\n\n# blr2_fit\n### AIC of 5987\n```\n:::\n\n\n\n## Results\n\n### Binary Logistic Regression\n\nModel 1 is a better predictor of species because it has a lower AIC (5195), a higher accuracy (92%) and a higher ROC (97%) compared to Model 2.\n\nFor Model 1, all variables are statistically significant at predicting species. For every unit increase in plant height, the log odds outcome will decrease by 0.029. Number of leaves has the greatest effect on the log odds of plant type. Results from the BLR are showcased in Table 1.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nblr1 <- glm(formula = f1, data = p_clean, family = binomial)\nsummary(blr1)\n# all p-values are small, so all coefficients are significant \n# for every unit increase in height the log odds outcome decreases by 0.0029\n\nblr1_fit <- blr_mdl %>%\n  fit(formula = f1, data = p_clean)\n\n\np_predict <- p_clean %>%\n  mutate(predict(blr1_fit, new_data = .))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# summary(blr1)\ntidy <- broom::tidy(blr1)\n\nnew_column_names <- c(\"Coefficient\", \"Estimate\", \"Standard Error\", \"Statistic\", \"P-Value\")\ncolnames(tidy) <- new_column_names\n\ntidy_table <- kable(tidy, align = \"c\") %>%\n  kable_styling(full_width = FALSE)\n\ntidy_table\n```\n\n::: {.cell-output-display}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Coefficient </th>\n   <th style=\"text-align:center;\"> Estimate </th>\n   <th style=\"text-align:center;\"> Standard Error </th>\n   <th style=\"text-align:center;\"> Statistic </th>\n   <th style=\"text-align:center;\"> P-Value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> (Intercept) </td>\n   <td style=\"text-align:center;\"> 3.2266851 </td>\n   <td style=\"text-align:center;\"> 0.1420708 </td>\n   <td style=\"text-align:center;\"> 22.71180 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> height </td>\n   <td style=\"text-align:center;\"> -0.0292173 </td>\n   <td style=\"text-align:center;\"> 0.0023061 </td>\n   <td style=\"text-align:center;\"> -12.66984 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> length </td>\n   <td style=\"text-align:center;\"> 0.0458233 </td>\n   <td style=\"text-align:center;\"> 0.0018661 </td>\n   <td style=\"text-align:center;\"> 24.55600 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> width </td>\n   <td style=\"text-align:center;\"> 0.0394434 </td>\n   <td style=\"text-align:center;\"> 0.0021000 </td>\n   <td style=\"text-align:center;\"> 18.78227 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> green_lvs </td>\n   <td style=\"text-align:center;\"> -1.9084747 </td>\n   <td style=\"text-align:center;\"> 0.0388634 </td>\n   <td style=\"text-align:center;\"> -49.10728 </td>\n   <td style=\"text-align:center;\"> 0 </td>\n  </tr>\n</tbody>\n</table>\n\n\n\nTable 1: Results from Model 1 binary logistic regression looking at plant height, length, width, and number of leaves. Includes coefficients, estiamte, standard errors for the coefficients, statistics, and p-value.\n:::\n:::\n\n\n\n### Predictions\n\nFor each species of palmetto, Table 2 shows how many plants in the original dataset would be correctly classified and how many were incorrectly classified by Model 1, as well as an the “% correctly classified”.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### MODEL 1\nsps_test1_predict <- p_clean %>%\n  ### straight up prediction, based on 50% prob threshold (to .pred_class):\n  mutate(predict(blr1_fit, new_data = p_clean)) %>%\n  ### but can also get the raw probabilities of class A vs B (.pred_A, .pred_B):\n  mutate(predict(blr1_fit, new_data = ., type = 'prob'))\n    ### note use of `.` as shortcut for \"the current dataframe\"\n\n\ntable(sps_test1_predict %>%\n        select(species, .pred_class))\n\naccuracy(sps_test1_predict, truth = species, estimate = .pred_class)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspecies <- c(\"S. repens\", \"S. etonia\")\npredict_correct <- c(5548, 5701)\npredict_incorrect <- c(564, 454)\npercent_correct <- c(91, 93)\n\nspecies_correct <- data.frame(species, predict_correct, predict_incorrect, percent_correct)\n\ncolumn_names <- c(\"Species\", \"# Correctly Predicted\", \"# Incorrectly Predicted\", \"Percent Correct\")\ncolnames(species_correct) <- column_names\n\nspecies_correct_kable <- kable(species_correct, align = \"c\") %>% \n  kable_styling()\n\nspecies_correct_kable\n```\n\n::: {.cell-output-display}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:center;\"> Species </th>\n   <th style=\"text-align:center;\"> # Correctly Predicted </th>\n   <th style=\"text-align:center;\"> # Incorrectly Predicted </th>\n   <th style=\"text-align:center;\"> Percent Correct </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:center;\"> S. repens </td>\n   <td style=\"text-align:center;\"> 5548 </td>\n   <td style=\"text-align:center;\"> 564 </td>\n   <td style=\"text-align:center;\"> 91 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:center;\"> S. etonia </td>\n   <td style=\"text-align:center;\"> 5701 </td>\n   <td style=\"text-align:center;\"> 454 </td>\n   <td style=\"text-align:center;\"> 93 </td>\n  </tr>\n</tbody>\n</table>\n\n\n\nTable 2: Shows how successfully Model 1 can “classify” a plant as the correct species. Shows how many plants in the original dataset would be correctly classified and how many were incorrectly classified by the model, also shows the percent correctly classified.\n:::\n:::\n\n\n\n## Conclusions\n\nModel 1 was a better predictor of species between the two models. Model 1 determined the species based on height, canopy length, canopy width, and number of leaves, which shows that canopy length does matter since Model 2 excluded this variable.\n\nModel 1 accurately predicted 91% of the observations for Saw Palmetto (*Serenoa repens*) and 93% for Scrub Palmetto (*Sabal etonia*).\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}