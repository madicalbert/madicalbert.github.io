[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Projects",
    "section": "",
    "text": "Benefits Transfer\n\n\n\nR\n\n\nCost Benefit Analysis\n\n\nBenefits Transfer\n\n\n\nEstimating restoration costs and storm protection benefits for 60 hectares of salt marsh habitat in Huntington Beach, CA using benefits transfer.\n\n\n\nMadison Calbert\n\n\nDec 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Logistic Regression\n\n\n\nR\n\n\nModeling\n\n\nRegression\n\n\nCross-Validation\n\n\nData Visualization\n\n\n\nUsing binary logistic regression to differentiate plant species.\n\n\n\nMadison Calbert\n\n\nMar 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-linear Least Squares\n\n\n\nR\n\n\nModeling\n\n\nData Visualization\n\n\n\nDescribing and predicting crop yields using non-linear least squares regression models.\n\n\n\nMadison Calbert\n\n\nMar 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Data Analysis\n\n\n\nR\n\n\nGeospatial\n\n\nData Visualization\n\n\n\nAnalyzing the spatial distribution of oil spill incidents in California from 2008.\n\n\n\nMadison Calbert\n\n\nFeb 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Sentiment Analysis\n\n\n\nR\n\n\nSentiment Analysis\n\n\nData Visualization\n\n\n\nExtracting text and performing sentiment analysis on Steinbeck’s East of Eden\n\n\n\nMadison Calbert\n\n\nFeb 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Analysis\n\n\n\nR\n\n\nTime Series\n\n\nData Visualization\n\n\n\nAssessing temporal patterns of salmon and steelhead trout in the Willamette Falls Fish Passage from 2001 to 2010.\n\n\n\nMadison Calbert\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling and Visualization\n\n\n\nR\n\n\nData Visualization\n\n\n\nExploring mountain yellow-legged frog (Rana muscosa) abundance in Sierra Lakes from 1995 - 2002.\n\n\n\nMadison Calbert\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "currently pursuing a Master of Environmental Science and Management degree with a specialization in conservation planning from the Bren School of Environmental Science and Management at UC Santa Barbara. I’m a wildlife biologist and botanist with three years’ experience conducting special status species surveys, botanical surveys, and biological construction monitoring."
  },
  {
    "objectID": "about.html#professional-affiliations",
    "href": "about.html#professional-affiliations",
    "title": "About",
    "section": "Professional Affiliations",
    "text": "Professional Affiliations\nCalifornia Native Plant Society | The Wildlife Society"
  },
  {
    "objectID": "posts/2024-02-02-data-viz/index.html",
    "href": "posts/2024-02-02-data-viz/index.html",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Adult Mountain yellow-legged frog in San Gabriel Mountains, Los Angeles County, CA. Photo Credit: Gary Nafis."
  },
  {
    "objectID": "posts/2024-02-02-data-viz/index.html#overview",
    "href": "posts/2024-02-02-data-viz/index.html#overview",
    "title": "Data Wrangling and Visualization",
    "section": "Overview",
    "text": "Overview\nThis report explores Mountain yellow-legged frog (Rana muscosa, RAMU) amphibian abundance data recorded by the Sierra Lakes Inventory Project. From the Environmental Data Initiative repository: “The Sierra Lakes Inventory Project (SLIP) was a research endeavor that ran from 1995-2002 and has supported research and management of Sierra Nevada aquatic ecosystems and their terrestrial interfaces. We described the physical characteristics of and surveyed aquatic communities for &gt;8,000 lentic water bodies in the southern Sierra Nevada, including lakes, ponds, marshes, and meadows.”"
  },
  {
    "objectID": "posts/2024-02-02-data-viz/index.html#ramu-abundance-by-year-and-life-stage-across-all-lakes",
    "href": "posts/2024-02-02-data-viz/index.html#ramu-abundance-by-year-and-life-stage-across-all-lakes",
    "title": "Data Wrangling and Visualization",
    "section": "RAMU Abundance by Year and Life Stage (across all lakes)",
    "text": "RAMU Abundance by Year and Life Stage (across all lakes)\n\nSteps\n\nLibraries\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(patchwork)\nlibrary(lubridate)\nlibrary(readxl)\nlibrary(janitor)\n\n\n\nLoad in the data.\nFilter for Rana muscosa and life stage. Lubridate to edit date and make year a factor.\nGroup by life stage and year and summarize to find Rana muscosa abundance.\n\n\n\nCode\nfrog_ds &lt;- read_excel(here('posts', '2024-02-02-data-viz','data', 'sierra_amphibians.xlsx')) %&gt;% \n  clean_names()\n\n\nramu_ds &lt;- frog_ds %&gt;% \n  select('survey_date', 'amphibian_species', 'amphibian_life_stage', 'amphibian_number') %&gt;% \n  filter(amphibian_species == 'RAMU', amphibian_life_stage != 'EggMass') %&gt;% \n  mutate(year = lubridate::year(survey_date))\n\n\nramu_ds$year &lt;- factor(ramu_ds$year)\n\n\nramu_count &lt;- ramu_ds %&gt;% \n  group_by(amphibian_life_stage, year) %&gt;% \n  summarise(amphibian_number = sum(amphibian_number, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\n\nPlot Rana muscosa abundance by life stage for each year.\n\n\n\nCode\nyear_plot &lt;- ggplot(data = ramu_count, aes(x = year, y = amphibian_number, fill = amphibian_life_stage)) + \n  geom_col() +\n  scale_fill_manual(values = c(\"darkgreen\", \"lightgreen\", \"#FED976\"))+\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10))+\n  labs(x = '',\n       y = 'Number of Amphibians',\n       fill = \"Life Stage\") + \n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n  #scale_y_log10() -- decided not to put it on the log scale"
  },
  {
    "objectID": "posts/2024-02-02-data-viz/index.html#ramu-abundance-by-lake-and-life-stage-across-all-years",
    "href": "posts/2024-02-02-data-viz/index.html#ramu-abundance-by-lake-and-life-stage-across-all-years",
    "title": "Data Wrangling and Visualization",
    "section": "RAMU Abundance by Lake and Life stage (across all years)",
    "text": "RAMU Abundance by Lake and Life stage (across all years)\n\nSteps\n\nFilter for Rana muscosa and life stage (adult and sub-adult only). Lubridate to edit date.\nGroup by life stage (adult and sub-adult) and lake and summarize to find Rana muscosa abundance. Find the top 5 lakes with the greatest RAMU abundance.\n\n\n\nCode\nadult_ds &lt;-frog_ds %&gt;% \n  select('survey_date', 'amphibian_species', 'amphibian_life_stage', 'amphibian_number', 'lake_id') %&gt;% \n  filter(amphibian_species == 'RAMU', \n         amphibian_life_stage != 'EggMass', \n         amphibian_life_stage != 'Tadpole') %&gt;% \n  mutate(year = lubridate::year(survey_date))\n\n\nadult_counts &lt;- adult_ds %&gt;% \n  group_by(lake_id) %&gt;% \n  summarise(amphibian_number = sum(amphibian_number, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\ntop_frogs &lt;- adult_counts %&gt;% top_n(5, wt = amphibian_number) %&gt;% \n  mutate(lake_id = paste(\"Lake\", lake_id, sep = \" \")) %&gt;% \n  mutate(lake_id = fct_reorder(lake_id, amphibian_number))\n\n\n\nPlot the RAMU abundance in the top 5 lakes.\n\n\n\nCode\nlake_plot &lt;- ggplot(data = top_frogs, aes(x = lake_id, y = amphibian_number)) + \n  geom_col(fill = \"#3CB371\") +\n  labs(x = '',\n       y = 'Number of Amphibians',\n       title = expression(\" \"),\n       subtitle = 'Adult + Subadult combined') +\n  theme_minimal()+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\nPut the Figure all together and make it pretty.\n\n\n\nCode\nfigure_1 &lt;- year_plot + lake_plot\nfigure_1 + plot_annotation(tag_levels = \"A\", title = \"Mountain yellow-legged frog Abundance\")\n\n\n\n\n\nFigure 1: Mountain yellow-legged frog (RAMU, Rana muscosa) Abundance. Plot A portrays RAMU abundance by year and life stages (adult, subadult, and tadpole) across all lakes. RAMU abundance increases over the years and the tadpoles account for the greatest number of amphibians across all years. In 2002, there is the greatest number of tadpoles. Plot B portrays RAMU abundance by lake and life stage (adult and subadult combined) across all years, including the top 5 lakes with the greatest species abundance."
  },
  {
    "objectID": "posts/2024-02-02-data-viz/index.html#works-cited",
    "href": "posts/2024-02-02-data-viz/index.html#works-cited",
    "title": "Data Wrangling and Visualization",
    "section": "Works Cited",
    "text": "Works Cited\nKnapp, R.A., C. Pavelka, E.E. Hegeman, and T.C. Smith. 2020. The Sierra Lakes Inventory Project: Non-Native fish and community composition of lakes and ponds in the Sierra Nevada, California ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/d835832d7fd00d9e4466e44eea87fab3"
  },
  {
    "objectID": "posts/2024-02-02-time-series/index.html",
    "href": "posts/2024-02-02-time-series/index.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Coho Salmon. Photo Credit: Native Fish Society."
  },
  {
    "objectID": "posts/2024-02-02-time-series/index.html#overview",
    "href": "posts/2024-02-02-time-series/index.html#overview",
    "title": "Time Series Analysis",
    "section": "Overview",
    "text": "Overview\nThis report describes the abundance of three species of fish (coho salmon, jack coho salmon, and steel-head salmon) from 2001 to 2010 in the Willamette Falls fish ladder passage on the Willamette river in Oregon. The abundance of salmon is visualized over the 10 year study for each species and by the total counts of each species for each year. Trends and seasonality in salmon abundance between species are discussed in this report."
  },
  {
    "objectID": "posts/2024-02-02-time-series/index.html#part-1-original-time-series",
    "href": "posts/2024-02-02-time-series/index.html#part-1-original-time-series",
    "title": "Time Series Analysis",
    "section": "Part 1: Original time series",
    "text": "Part 1: Original time series\nOver the last 10 years, salmon abundance in the Willamette Falls fish passage has clear seasonality and oscillations for the three species. Coho salmon abundance has increased over the 10 year span, while Jack-coho and steelhead remained consistent as shown in Figure 1, Plot A. In any given year, coho and jack-coho salmon abundances peak in the fall months (Sept - Nov) and the steelhead salmon abundance peaks in the summer (May - July) as shown in Figure 1, Plot B.\n\nSteps\n\nLibraries\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(patchwork)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(fable)\nlibrary(RColorBrewer)\n\n\n\nLoad and tidy data\nMake into a time series\nPivot longer\n\n\n\nCode\nfish_df &lt;- read_csv(here('posts', '2024-02-02-time-series','data', 'willamette_fish_passage.csv')) %&gt;% \n  clean_names()\n\nsalmon_df &lt;- fish_df %&gt;% \n  select('project', 'date', 'coho', 'jack_coho', 'steelhead') %&gt;% \n  replace_na(replace = list(coho = 0, jack_coho = 0, steelhead = 0))\n\nsalmon_ts &lt;- salmon_df %&gt;% \n  mutate(date = lubridate::mdy(date)) %&gt;% \n  as_tsibble(key = NULL, \n             index = date)\n\nsalmon_ts_pivot &lt;- salmon_ts %&gt;% \n  pivot_longer(cols = c('coho', 'jack_coho', 'steelhead'),\n                 names_to = \"species\",\n                 values_to = \"count\") \n\n\n\nMake a pretty plot\n\n\n\nCode\nplot_ts &lt;- ggplot(data = salmon_ts_pivot, aes(x = date, y = count, color = species)) +\n  geom_line() + \n  labs(x = \" \",\n       y = 'Abundance',\n       color = \"Species\",\n       subtitle = \"2001 to 2010\")+\n  theme_minimal() +\n  scale_x_date(date_breaks = \"1 year\", date_labels = \"%Y\")+\n  scale_color_manual(values = c(\"darkgreen\", \"red\", \"#999999\"),\n                    labels = c(\"Coho\", \"Jack Coho\", \"Steelhead\"))+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\nIsolate from 2009 to 2011 and make an additional plot for clarity.\n\n\n\nCode\nsalmon_ts_filter &lt;- salmon_ts_pivot %&gt;% \n  filter_index(\"2009-01-01\" ~ \".\")\n\nzoom_plot_ts &lt;- ggplot(data = salmon_ts_filter, aes(x = date, y = count, color = species)) +\n  geom_line() + \n  labs(x = \" \",\n       y = 'Abundance',\n       color = \"Species\",\n       subtitle = \"2009 to 2010\")+\n  theme_minimal() +\n  scale_x_date(date_breaks = \"2 month\", date_labels = \"%b\")+\n  scale_color_manual(values = c(\"darkgreen\", \"red\", \"#999999\"),\n                    labels = c(\"Coho\", \"Jack Coho\", \"Steelhead\"))+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\nPut the plots together in one figure.\n\n\n\nCode\nfigure_1 &lt;- plot_ts / zoom_plot_ts\nfigure_1 + plot_annotation(tag_levels = \"A\")\n\n\n\n\n\nFigure 1: Salmon Abundance in Willamette Falls fish passage from 2001 to 2010. Plot A portrays salmon abundance across all the years. There are clear oscillations and seasonality across all three species (Coho, Jack-coho, and Steelhead salmon). Plot B highlights a subset of the data, focusing on salmon abundance from 2009 to 2010 to better portray the seasonality across the three species for a given year. The coho and jack-coho salmon abundances peak in the fall months (Sept - Nov) and the steelhead abundance peaks in the summer (May - July)."
  },
  {
    "objectID": "posts/2024-02-02-time-series/index.html#part-2-seasonplot",
    "href": "posts/2024-02-02-time-series/index.html#part-2-seasonplot",
    "title": "Time Series Analysis",
    "section": "Part 2: Seasonplot",
    "text": "Part 2: Seasonplot\nAs shown in Figure 2, the coho salmon abundance peaks in the fall months from Sept. to Nov. There is a clear increase in the species population size from 2001 to 2010. Likewise, the jack-coho abundance spikes in the fall months and also has an increase in species abundance over the ten years span. In comparison, Steelhead salmon abundance more steadily rises from January through July. There numbers appear to be declining over the years.\n\nSteps\n\nMake a seasonplot"
  },
  {
    "objectID": "posts/2024-02-02-time-series/index.html#part-3-annual-counts-by-species",
    "href": "posts/2024-02-02-time-series/index.html#part-3-annual-counts-by-species",
    "title": "Time Series Analysis",
    "section": "Part 3: Annual counts by species",
    "text": "Part 3: Annual counts by species\nAs shown in Figure 3, there are clear trends in annual salmon counts for each species from 2001 to 2010. The Steelhead salmon have the greatest abundance out of the three species and have an overall declining trend with a increase from 2009 to 2010. Coho salmon remain consistent from 2001 to 2008 and then have a large increase in their abundance. And Jack coho salmon have the lowest species abundance across the years and remain at a constant size.”\n\nSteps\n\nGroup by year and species to find total abundances.\n\n\n\nCode\nfish_year &lt;- salmon_ts_pivot %&gt;%\n  index_by(year = ~year (.)) %&gt;%\n  group_by(year, species) %&gt;%\n  summarise(count = sum(count, na.rm = TRUE)) %&gt;%\n  ungroup\n\n\n\nMake a pretty plot.\n\n\n\nCode\nyear_plot2 &lt;- ggplot(data = fish_year, aes(x = year, y = count, color = species)) + \n  geom_line() +\n  scale_color_manual(values = c(\"darkgreen\", \"red\", \"#999999\"),\n                    labels = c(\"Coho\", \"Jack Coho\", \"Steelhead\")) +\n  labs(x = '',\n       y = 'Abundance', \n       color = \"Species\")+ \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  scale_x_continuous(breaks = fish_year$year)\nyear_plot2\n\n\n\n\n\nFigure 3: Salmon Abundance per Year from 2001 to 2010. The figure portray the trends in annual salmon abundance totals for the three species. The Steelhead salmon have the greatest abundance out of the three species and have an overall declining trend with a increase from 2009 to 2010. Coho salmon remain consistent from 2001 to 2008 and then have a large increase in their abundance. And Jack coho have the lowest species abundance across the years and remain at a constant size."
  },
  {
    "objectID": "posts/2024-02-02-time-series/index.html#works-cited",
    "href": "posts/2024-02-02-time-series/index.html#works-cited",
    "title": "Time Series Analysis",
    "section": "Works Cited",
    "text": "Works Cited\nU.S. Army Corps of Engineers, NWD; Chelan, Douglas, and Grant County PUDs; Yakima Klickitat Fisheries Project; Colville Tribes Fish & Wildlife (OBMEP); Oregon Department of Fish & Wildlife; Washington Department of Fish & Wildlife. DART Adult Passage Counts Graphics & Text. Columbia Basin Research, Univeristy of Washington. Accessed on January 25, 2024. https://www.cbr.washington.edu/dart/query/adult_graph_text"
  },
  {
    "objectID": "posts/2024-02-10-sentiment-analysis/index.html",
    "href": "posts/2024-02-10-sentiment-analysis/index.html",
    "title": "Text Sentiment Analysis",
    "section": "",
    "text": "First edition dust cover jacket of East of Eden (1952) by American author John Steinbeck. Photo retrieved from Wikipedia page."
  },
  {
    "objectID": "posts/2024-02-10-sentiment-analysis/index.html#overview",
    "href": "posts/2024-02-10-sentiment-analysis/index.html#overview",
    "title": "Text Sentiment Analysis",
    "section": "Overview",
    "text": "Overview\nIn this report, I analyze John Steinbeck’s classic novel, “East of Eden”, to determine the most frequently used words and overall sentiments throughout the book. Figure 1 displays the most frequently used words in each chapter for Chapters 1 - 16, and Figure 2 visualizes a cloud map of most frequenlty used word for just Chapter 1. Figures 3 and 4 portrays the sentiment analysis for Chapters 1-16."
  },
  {
    "objectID": "posts/2024-02-10-sentiment-analysis/index.html#text-analysis",
    "href": "posts/2024-02-10-sentiment-analysis/index.html#text-analysis",
    "title": "Text Sentiment Analysis",
    "section": "Text Analysis",
    "text": "Text Analysis\n\nLoad libraries needed for analysis\nLoad in the pdf of “East of Eden”\nConvert the pdf text into a data frame\nWrangle the data to get tokens into tidy format and remove stop words\nDetermine word counts by chapter\nPlot it in a bar graph\n\n\n\nCode\n### Load Packages\n\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(pdftools)\nlibrary(ggwordcloud)\nlibrary(textdata)\n\n\n\n\nCode\n### Get East of Eden Script loaded in\n\neoe_text &lt;- pdftools::pdf_text(here::here('posts', '2024-02-10-sentiment-analysis','data', 'East-of-Eden.pdf'))\n\n\n\n\nCode\n### Get PDF script text into a data frame\n\neoe_lines &lt;- data.frame(eoe_text) %&gt;% \n  mutate(page = 1:n()) %&gt;%\n  mutate(text_full = str_split(eoe_text, pattern = '\\\\n')) %&gt;% \n  unnest(text_full) %&gt;% \n  mutate(text_full = str_trim(text_full)) \n\n\n\n\nCode\neoe_chapts &lt;- eoe_lines %&gt;% \n  slice(-(1:247)) %&gt;% \n  mutate(chapter = ifelse(str_detect(text_full, \"Chapter\"), text_full, NA)) %&gt;% \n  fill(chapter, .direction = 'down') %&gt;% \n  separate(col = chapter, into = c(\"ch\", \"num\"), sep = \" \") %&gt;% \n  mutate(chapter = as.numeric(as.roman(num)))\n\n\n\n\nCode\neoe_words &lt;- eoe_chapts %&gt;% \n  unnest_tokens(word, text_full) %&gt;% \n  select(-eoe_text)\n\n\n\n\nCode\neoe_wordcount &lt;- eoe_words %&gt;% \n  count(chapter, word)\n\nwordcount_clean &lt;- eoe_wordcount %&gt;% \n  anti_join(stop_words, by = 'word')\n\n\n\n\nCode\ntop_5_words &lt;- wordcount_clean %&gt;% \n  filter(chapter == 1:16) %&gt;% \n  group_by(chapter) %&gt;% \n  arrange(-n) %&gt;% \n  slice(1:5) %&gt;%\n  ungroup()\n\n# Make some graphs: \nggplot(data = top_5_words, aes(x = n, y = word)) +\n  geom_col(fill = \"darkgreen\") +\n  facet_wrap(~chapter, scales = \"free\")+\n  theme_minimal()+\n  labs(x = \" \", \n       y = \" \")\n\n\n\n\n\nFigure 1: Top 5 Words per Chapter. Figure 1 portrays the top 5 most frequently used words per chapter for chapters 1 through 16. The most commonly used word is ‘Adam’ in Chapter 10."
  },
  {
    "objectID": "posts/2024-02-10-sentiment-analysis/index.html#word-cloud",
    "href": "posts/2024-02-10-sentiment-analysis/index.html#word-cloud",
    "title": "Text Sentiment Analysis",
    "section": "Word Cloud",
    "text": "Word Cloud\nCreate a visualization for most frequently used words in Chapter 1:\n\nFilter for only Chapter 1\nDetermine the top 100 most used words\nPlot it\n\n\n\nCode\nch1_top100 &lt;- wordcount_clean %&gt;% \n  filter(chapter == 1) %&gt;% \n  arrange(-n) %&gt;% \n  slice(1:100)\n\n\n\n\nCode\nch1_cloud &lt;- ggplot(data = ch1_top100, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n), shape = \"diamond\") +\n  scale_size_area(max_size = 6) +\n  scale_color_gradientn(colors = c(\"darkgreen\",\"blue\",\"purple\")) +\n  theme_minimal()\n\nch1_cloud\n\n\n\n\n\nFigure 2: East of Eden Chapter 1 Word Cloud. Fig 2 visualzes the most frequently used words from Chapter 1. As the novel starts out with a thorough description of the setting, the Salinas Valley, there are a lot of words describing the geographical location and physical attributes of Central California."
  },
  {
    "objectID": "posts/2024-02-10-sentiment-analysis/index.html#sentiment-analysis",
    "href": "posts/2024-02-10-sentiment-analysis/index.html#sentiment-analysis",
    "title": "Text Sentiment Analysis",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nDetermine the overall sentiment (positive or negative) for each chapter:\n\nLoad in the affin lexicon\nBind the affin words to EOE words\nDetermine positive and negative word counts\nPlot it by chapter\nConsider the log ratios and plot it\n\n\n\nCode\nafinn_lex &lt;- get_sentiments(lexicon = \"afinn\")\n### you may be prompted to download an updated lexicon - say yes!\n\n# Let's look at the pretty positive words:\nafinn_pos &lt;- get_sentiments(\"afinn\") %&gt;% \n  filter(value %in% c(3,4,5))\n\n\n\n\nCode\neoe_afinn &lt;- eoe_words %&gt;% \n  filter(chapter == 1:16) %&gt;% \n  inner_join(afinn_lex, by = 'word') ### why inner_join?\n\n\n\n\nCode\nbing_lex &lt;- get_sentiments(lexicon = \"bing\")\n\nnrc_lex &lt;- get_sentiments(lexicon = \"nrc\")\n\neoe_bing &lt;- eoe_words %&gt;% \n  filter(chapter == 1:16) %&gt;% \n  inner_join(bing_lex, by = 'word')\n\nbing_counts &lt;- eoe_bing %&gt;% \n  group_by(chapter, sentiment) %&gt;%\n  summarize(n = n())\n\n# Plot them: \nggplot(data = bing_counts, aes(x = sentiment, y = n, fill = sentiment)) +\n  geom_col() +\n  facet_wrap(~chapter) + \n  labs(\n    x = \" \", \n    y = \"Count\", \n    fill = \"Sentiment\")+\n  scale_fill_manual(values = c(\"positive\" = \"slateblue\", \"negative\" = \"darkred\"))+\n  theme_minimal()\n\n\n\n\n\nFigure 3: East of Eden Sentiment Analysis. Fig 3 portrays the number of words per chapter that are associated with positive or negative connotations. Chapter 3 has the most negative words and Chapter 7 has the most positive words.\n\n\n\n\n\n\nCode\n# find log ratio score overall:\nbing_log_ratio_book &lt;- eoe_bing %&gt;% \n  summarize(n_pos = sum(sentiment == 'positive'),\n            n_neg = sum(sentiment == 'negative'),\n            log_ratio = log(n_pos / n_neg))\n\n# Find the log ratio score by chapter: \nbing_log_ratio_ch &lt;- eoe_bing %&gt;% \n  group_by(chapter) %&gt;% \n  summarize(n_pos = sum(sentiment == 'positive'),\n            n_neg = sum(sentiment == 'negative'),\n            log_ratio = log(n_pos / n_neg)) %&gt;%\n  mutate(log_ratio_adjust = log_ratio - bing_log_ratio_book$log_ratio) %&gt;%\n  mutate(pos_neg = ifelse(log_ratio_adjust &gt; 0, 'pos', 'neg'))\n\nggplot(data = bing_log_ratio_ch, \n       aes(x = log_ratio_adjust,\n           y = fct_rev(factor(chapter)),\n           fill = pos_neg)) +\n           # y = fct_rev(as.factor(chapter)))) +\n  geom_col() +\n  labs(x = 'Adjusted log(positive/negative)',\n       y = 'Chapter number') +\n  scale_fill_manual(values = c('pos' = 'slateblue', 'neg' = 'darkred')) +\n  theme_minimal() +\n  theme(legend.position = 'none')\n\n\n\n\n\nFigure 4: Ratio of Sentiment Analysis. Fig 4 portrays the average sentiment for the whole book and compares each chapter to that average. Based on a log scale, a neutral chapter will equal 0 (ratio = 1), and a super positive chapter will show the same length as a super negative chapter but in the opposite directions. Chapter 2 was the most positive, and Chapters 3 and 9 were very negative."
  },
  {
    "objectID": "posts/2024-02-10-sentiment-analysis/index.html#works-cited",
    "href": "posts/2024-02-10-sentiment-analysis/index.html#works-cited",
    "title": "Text Sentiment Analysis",
    "section": "Works Cited",
    "text": "Works Cited\nSteinbeck, John. East of Eden. Penguin Classics, 2000. Retreived from: https://hitalki.org/blog/wp-content/uploads/2023/05/East-of-Eden.pdf"
  },
  {
    "objectID": "posts/2024-03-03-binary-log-regression/index.html",
    "href": "posts/2024-03-03-binary-log-regression/index.html",
    "title": "Binary Logistic Regression",
    "section": "",
    "text": "Serenoa repens is a flowering, perennial, wetland shrub that is commonly found in wet to dry flatwoods and hammocks throughout the state of Florida."
  },
  {
    "objectID": "posts/2024-03-03-binary-log-regression/index.html#overview",
    "href": "posts/2024-03-03-binary-log-regression/index.html#overview",
    "title": "Binary Logistic Regression",
    "section": "Overview",
    "text": "Overview\nSaw Palmetto (Serenoa repens) and Scrub Palmetto (Sabal etonia) are both species of palms native to Florida. In this report, I use binary logistic regression to test the feasibility of using different plant characteristics to differentiate between the two species. The different plant variables include plant height (height), canopy length (length), canopy width (width), and number of green leaves (green_lvs)."
  },
  {
    "objectID": "posts/2024-03-03-binary-log-regression/index.html#data-source",
    "href": "posts/2024-03-03-binary-log-regression/index.html#data-source",
    "title": "Binary Logistic Regression",
    "section": "Data Source",
    "text": "Data Source\nThe data for this analysis was sourced from Environmental Data Initiative Data Portal. This data package is comprised of three datasets all pertaining to two dominant palmetto species, Serenoa repens and Sabal etonia, at Archbold Biological Station in south-central Florida.\nData source: Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5"
  },
  {
    "objectID": "posts/2024-03-03-binary-log-regression/index.html#pseudocode",
    "href": "posts/2024-03-03-binary-log-regression/index.html#pseudocode",
    "title": "Binary Logistic Regression",
    "section": "Pseudocode",
    "text": "Pseudocode\n\nLoad libraries, load data, clean/tidy the data, select desired variables (species, height, length, width, green_lvs)\nData visualization to hypothesize which variables differ between species.\nBuild the two models: Model 1 determines species based on height, length, width, and number of leaves; & Model 2 determines species based on height, width, and number of leaves (excluding length).\nRun the binary logistic regression on both models, using generalized linear model.\nSplit the data into testing group and training group.\nInitialize workflow\nApply workflow to folded training data set for both models.\nTrain the whole data set with the best model to determine which variables best predict species.\n\n\n\nCode\n### Load libraries\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tidymodels)\n\nlibrary(cowplot)\nlibrary(kableExtra)\nlibrary(broom)\n\n\n### Load in the data\n\np_df &lt;- read_csv(here('posts', '2024-03-03-binary-log-regression','data', 'palmetto.csv'))\n\n\n### Tidy the data and make species a factor\n\np_clean &lt;- p_df %&gt;% \n  select(species, height, length, width, green_lvs) %&gt;% \n  mutate(species = as_factor(species)) %&gt;% \n  drop_na()"
  },
  {
    "objectID": "posts/2024-03-03-binary-log-regression/index.html#methods",
    "href": "posts/2024-03-03-binary-log-regression/index.html#methods",
    "title": "Binary Logistic Regression",
    "section": "Methods",
    "text": "Methods\n\nData Visualization\n\n\nCode\nlvs_bp &lt;- ggplot(p_clean, aes(x = as_factor(species), y = green_lvs)) + \n  geom_boxplot(fill = \"lightgreen\") + \n  labs(x = \" \", \n       y = \"Number of Leaves\") +\n  theme_minimal()+\n  scale_x_discrete(labels = c(\"S. repens\", \"S. etonia\"))\n\nheight_bp &lt;- ggplot(p_clean, aes(x = as_factor(species), y = height)) + \n  geom_boxplot(fill = \"lightgreen\")+ \n  labs(x = \" \", \n       y = \"Height (cm)\") +\n  theme_minimal()+\n  scale_x_discrete(labels = c(\"S. repens\", \"S. etonia\"))\n\nlength_bp &lt;- ggplot(p_clean, aes(x = as_factor(species), y = length)) + \n  geom_boxplot(fill = \"lightgreen\")+ \n  labs(x = \" \", \n       y = \"Canopy Length (cm)\") +\n  theme_minimal()+\n  scale_x_discrete(labels = c(\"S. repens\", \"S. etonia\"))\n\nwidth_bp &lt;- ggplot(p_clean, aes(x = as_factor(species), y = width)) + \n  geom_boxplot(fill = \"lightgreen\")+ \n  labs(x = \" \", \n       y = \"Canopy Width (cm)\") +\n  theme_minimal()+\n  scale_x_discrete(labels = c(\"S. repens\", \"S. etonia\"))\n\n### put it all together now \n\ncombined_plot &lt;- plot_grid(\n  lvs_bp, height_bp,\n  length_bp, width_bp,\n  ncol = 2  \n)\n\nprint(combined_plot)\n\n\n\n\n\nFigure 1: Saw Palmetto (Serenoa repens) and Scrub Palmetto (Sabal etonia) differ by the number of leaves and are fairly similar based on height, canopy width, and canopy length. Number of leaves appears to be the best predictor variable to differentiate these two palmetto species.\n\n\n\n\n\n\nDefine the models\n\nModel 1: Species as a function of height, canopy length, canopy width, and number of leaves\nModel 2: Species as a function of height, canopy width, and number of leaves\n\n\n\nCode\n### sps. 1 = s. repens\n### sps. 2 = s. etonia\n\nf1 &lt;- species ~ height + length + width + green_lvs \nf2 &lt;- species ~ height + width + green_lvs\n\n\n\n\nCrossfold Validation\nModel 1 has an accuracy of 92%, this is the percent classified correctly in the testing group. In addition, the ROC is 97% which is good and explains that we do not have a lot of false positives. We are close to a perfect classifier.\n\n\nCode\np_clean %&gt;%\n  group_by(species) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(prop = n / sum(n))\n\nset.seed(10101)\np_folds &lt;- vfold_cv(p_clean, v = 10, repeats = 10)\n\n\nblr_mdl &lt;- logistic_reg() %&gt;%\n  set_engine('glm') ### this is the default - we could try engines from other packages or functions\n\n\nblr_wf &lt;- workflow() %&gt;%   ### initialize workflow\n  add_model(blr_mdl) %&gt;%\n  add_formula(formula = f1)\n\n\nblr_fit_folds &lt;- blr_wf %&gt;%\n  fit_resamples(p_folds)\n\n\n### Average the predictive performance of the ten models:\n# collect_metrics(blr_fit_folds)\n\n\nModel 2 has an accuracy of 89%, so this model is not as accurate of a predictor compares to Model 1. In addition the ROC is 96% which is still a good value, but not as strong as Model 1.\n\n\nCode\nblr2_wf &lt;- workflow() %&gt;%   ### initialize workflow\n  add_model(blr_mdl) %&gt;%\n  add_formula(formula = f2)\n\n\nblr2_fit_folds &lt;- blr2_wf %&gt;%\n  fit_resamples(p_folds)\n\n\n### Average the predictive performance of the ten models:\n# collect_metrics(blr2_fit_folds)\n\n\n\n\nCheck AIC\nModel 1 has a lower AIC compared to Model 2, so Model 1 is a better predictor of species.\n\n\nCode\nblr1_fit &lt;- blr_mdl %&gt;%\n  fit(formula = f1, data = p_clean)\n\nblr2_fit &lt;- blr_mdl %&gt;%\n  fit(formula = f2, data = p_clean)\n\n# blr1_fit\n### AIC of 5195\n\n# blr2_fit\n### AIC of 5987"
  },
  {
    "objectID": "posts/2024-03-03-binary-log-regression/index.html#results",
    "href": "posts/2024-03-03-binary-log-regression/index.html#results",
    "title": "Binary Logistic Regression",
    "section": "Results",
    "text": "Results\n\nBinary Logistic Regression\nModel 1 is a better predictor of species because it has a lower AIC (5195), a higher accuracy (92%) and a higher ROC (97%) compared to Model 2.\nFor Model 1, all variables are statistically significant at predicting species. For every unit increase in plant height, the log odds outcome will decrease by 0.029. Number of leaves has the greatest effect on the log odds of plant type. Results from the BLR are showcased in Table 1.\n\n\nCode\nblr1 &lt;- glm(formula = f1, data = p_clean, family = binomial)\nsummary(blr1)\n# all p-values are small, so all coefficients are significant \n# for every unit increase in height the log odds outcome decreases by 0.0029\n\nblr1_fit &lt;- blr_mdl %&gt;%\n  fit(formula = f1, data = p_clean)\n\n\np_predict &lt;- p_clean %&gt;%\n  mutate(predict(blr1_fit, new_data = .))\n\n\n\n\nCode\n# summary(blr1)\ntidy &lt;- broom::tidy(blr1)\n\nnew_column_names &lt;- c(\"Coefficient\", \"Estimate\", \"Standard Error\", \"Statistic\", \"P-Value\")\ncolnames(tidy) &lt;- new_column_names\n\ntidy_table &lt;- kable(tidy, align = \"c\") %&gt;%\n  kable_styling(full_width = FALSE)\n\ntidy_table\n\n\n\n\n\n\nCoefficient\n\n\nEstimate\n\n\nStandard Error\n\n\nStatistic\n\n\nP-Value\n\n\n\n\n\n\n(Intercept)\n\n\n3.2266851\n\n\n0.1420708\n\n\n22.71180\n\n\n0\n\n\n\n\nheight\n\n\n-0.0292173\n\n\n0.0023061\n\n\n-12.66984\n\n\n0\n\n\n\n\nlength\n\n\n0.0458233\n\n\n0.0018661\n\n\n24.55600\n\n\n0\n\n\n\n\nwidth\n\n\n0.0394434\n\n\n0.0021000\n\n\n18.78227\n\n\n0\n\n\n\n\ngreen_lvs\n\n\n-1.9084747\n\n\n0.0388634\n\n\n-49.10728\n\n\n0\n\n\n\n\nTable 1: Results from Model 1 binary logistic regression looking at plant height, length, width, and number of leaves. Includes coefficients, estiamte, standard errors for the coefficients, statistics, and p-value.\n\n\n\n\nPredictions\nFor each species of palmetto, Table 2 shows how many plants in the original dataset would be correctly classified and how many were incorrectly classified by Model 1, as well as an the “% correctly classified”.\n\n\nCode\n### MODEL 1\nsps_test1_predict &lt;- p_clean %&gt;%\n  ### straight up prediction, based on 50% prob threshold (to .pred_class):\n  mutate(predict(blr1_fit, new_data = p_clean)) %&gt;%\n  ### but can also get the raw probabilities of class A vs B (.pred_A, .pred_B):\n  mutate(predict(blr1_fit, new_data = ., type = 'prob'))\n    ### note use of `.` as shortcut for \"the current dataframe\"\n\n\ntable(sps_test1_predict %&gt;%\n        select(species, .pred_class))\n\naccuracy(sps_test1_predict, truth = species, estimate = .pred_class)\n\n\n\n\nCode\nspecies &lt;- c(\"S. repens\", \"S. etonia\")\npredict_correct &lt;- c(5548, 5701)\npredict_incorrect &lt;- c(564, 454)\npercent_correct &lt;- c(91, 93)\n\nspecies_correct &lt;- data.frame(species, predict_correct, predict_incorrect, percent_correct)\n\ncolumn_names &lt;- c(\"Species\", \"# Correctly Predicted\", \"# Incorrectly Predicted\", \"Percent Correct\")\ncolnames(species_correct) &lt;- column_names\n\nspecies_correct_kable &lt;- kable(species_correct, align = \"c\") %&gt;% \n  kable_styling()\n\nspecies_correct_kable\n\n\n\n\n\n\nSpecies\n\n\n# Correctly Predicted\n\n\n# Incorrectly Predicted\n\n\nPercent Correct\n\n\n\n\n\n\nS. repens\n\n\n5548\n\n\n564\n\n\n91\n\n\n\n\nS. etonia\n\n\n5701\n\n\n454\n\n\n93\n\n\n\n\nTable 2: Shows how successfully Model 1 can “classify” a plant as the correct species. Shows how many plants in the original dataset would be correctly classified and how many were incorrectly classified by the model, also shows the percent correctly classified."
  },
  {
    "objectID": "posts/2024-03-03-binary-log-regression/index.html#conclusions",
    "href": "posts/2024-03-03-binary-log-regression/index.html#conclusions",
    "title": "Binary Logistic Regression",
    "section": "Conclusions",
    "text": "Conclusions\nModel 1 was a better predictor of species between the two models. Model 1 determined the species based on height, canopy length, canopy width, and number of leaves, which shows that canopy length does matter since Model 2 excluded this variable.\nModel 1 accurately predicted 91% of the observations for Saw Palmetto (Serenoa repens) and 93% for Scrub Palmetto (Sabal etonia)."
  },
  {
    "objectID": "posts/2024-02-10-spatial-data/index.html",
    "href": "posts/2024-02-10-spatial-data/index.html",
    "title": "Spatial Data Analysis",
    "section": "",
    "text": "Oil on the beach at Refugio State Park in Santa Barbara, California, on May 19, 2015. (U.S. Coast Guard)."
  },
  {
    "objectID": "posts/2024-02-10-spatial-data/index.html#overview",
    "href": "posts/2024-02-10-spatial-data/index.html#overview",
    "title": "Spatial Data Analysis",
    "section": "Overview",
    "text": "Overview\nIn this report, I explore oil spill incidents throughout all 58 California counties in the year 2008. I used data from CA Department of Fish and Wildlife (CDFW) Oil Spill Incident Tracking [ds394] that was published on July 29, 2009. “The Office of Spill Prevention and Response (OSPR) Incident Tracking Database is a statewide oil spill tracking information system. The data are collected by OSPR Field Response Team members for Marine oil spills and by OSPR Inland Pollution Coordinators and Wardens for Inland incidents. An ‘incident’, for purposes of this database, is a discharge or threatened discharge of petroleum or other deleterious material into the waters of the state.” The purpose of this analysis is to develop a better understanding of which California counties have the most oil spill incidents and to spatially visualize the oil spill incidents."
  },
  {
    "objectID": "posts/2024-02-10-spatial-data/index.html#spatial-data-visualization",
    "href": "posts/2024-02-10-spatial-data/index.html#spatial-data-visualization",
    "title": "Spatial Data Analysis",
    "section": "Spatial Data Visualization",
    "text": "Spatial Data Visualization\nCreate an interactive map displaying all oil spill incidents in each CA county:\n\nLoad all Libraries needed for this analysis.\nLoad the California county data shape files and the oil spill data sets.\nConvert the oil spill data set into a shape file and ensure it has the same coordinate reference system (CRS) as the California counties shape file.\nPerform a spatial join of CA counties over oil spill points.\nMake an interactive map of oil spill incidents by county.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(sf) \nlibrary(terra) \nlibrary(tidyterra) \nlibrary(gstat)\nlibrary(stars)\nlibrary(janitor)\nlibrary(tmap)\nlibrary(spatstat)\n\n\n\n\nCode\n### spatial join \n\noil_ca_sf &lt;- st_join(oil_sf, ca_counties_sf) ### points over counties\nca_oil_sf &lt;- st_join(ca_counties_sf, oil_sf) ### counties over points"
  },
  {
    "objectID": "posts/2024-02-10-spatial-data/index.html#chloropleth-map",
    "href": "posts/2024-02-10-spatial-data/index.html#chloropleth-map",
    "title": "Spatial Data Analysis",
    "section": "Chloropleth Map",
    "text": "Chloropleth Map\nDetermine the number of oil spill incidents in each county to display the counties with the greatest amount of oil spill occurrences:\n\nGroup the oil spill data by county and summarize the number of oil spills in each county.\nCreate the chloropleth map.\n\n\n\nCode\n### group by county and summarize \n\noil_counts_sf &lt;- ca_oil_sf %&gt;% \n  mutate(county = name) %&gt;% \n  group_by(county) %&gt;%\n  summarize(oil_counts = n())\n\n\n\n\nCode\n### chloropleth map \n\nggplot(data = oil_counts_sf) +\n  geom_sf(aes(fill = oil_counts), color = \"white\", size = 0.1) +\n  scale_fill_gradientn(colors = c(\"lightgray\",\"orange\",\"red\")) +\n  theme_minimal() +\n  labs(fill = \"Number of oil spill incidents\")\n\n\n\n\n\nFigure 2: Abundance of Oil Spill Incidents in 2008 in CA. This figure displays that the greatest number of oil spill incidents are occuring in Los Angeles and San Diego counties."
  },
  {
    "objectID": "posts/2024-02-10-spatial-data/index.html#point-pattern-analysis",
    "href": "posts/2024-02-10-spatial-data/index.html#point-pattern-analysis",
    "title": "Spatial Data Analysis",
    "section": "Point Pattern Analysis",
    "text": "Point Pattern Analysis\nPerform a point pattern analysis to assess whether oil spills tend to be more clustered or more uniform than complete spatial randomness:\n\nConvert oil observations to spatial point pattern\nConvert county boundaries to observation window\nCombine as a point pattern object (points + window)\nPlot it\n\n\n\nCode\n### Convert oil observations to spatial point pattern\noil_ppp &lt;- as.ppp(oil_sf) \n\n### Convert county boundary to observation window\nca_counties_win &lt;- as.owin(ca_counties_sf) \n\n### Combine as a point pattern object (points + window):\noil_full &lt;- ppp(oil_ppp$x, oil_ppp$y, window = ca_counties_win)\n\nplot(oil_full, main = \"Oil Spill Incidents from 2008\") \n\n\n\n\n\nFigure 3: Point Pattern Analysis of Oil Spills in CA from 2008. This figure portrays that San Francisco, Los Angeles, and San Diego counties have the highest occurences of oil spills and clustering. The plus sign denotes illegal point or oils spills that fall outside the window.\n\n\n\n\nPlot the G function to determine clustering:\n\nMake a sequence of distances over which you’ll calculate G(r)\nCalculate the actual and theoretical G(r) values, using 100 simulations of CSR for the “theoretical” outcome\nCheck the output of gfunction, then gather this to plot series in ggplot, then make a graph in ggplot\n\n\n\nCode\n### Make a sequence of distances over which you'll calculate G(r)\nr_vec &lt;- seq(0, 10000, by = 100) \n\ngfunction_out &lt;- envelope(oil_full, fun = Gest, r = r_vec, \n                          nsim = 100, verbose = FALSE) \n### Calculate the actual and theoretical G(r) values, using 100 \n### simulations of CSR for the \"theoretical\" outcome\n\ngfunction_out ### Check the output of gfunction, then...\n\n\nPointwise critical envelopes for G(r)\nand observed value for 'oil_full'\nEdge correction: \"km\"\nObtained from 100 simulations of CSR\nAlternative: two.sided\nSignificance level of pointwise Monte Carlo test: 2/101 = 0.0198\n.....................................................................\n     Math.label     Description                                      \nr    r              distance argument r                              \nobs  hat(G)[obs](r) observed value of G(r) for data pattern          \ntheo G[theo](r)     theoretical value of G(r) for CSR                \nlo   hat(G)[lo](r)  lower pointwise envelope of G(r) from simulations\nhi   hat(G)[hi](r)  upper pointwise envelope of G(r) from simulations\n.....................................................................\nDefault plot formula:  .~r\nwhere \".\" stands for 'obs', 'theo', 'hi', 'lo'\nColumns 'lo' and 'hi' will be plotted as shading (by default)\nRecommended range of argument r: [0, 6100]\nAvailable range of argument r: [0, 10000]\n\n\nCode\n### Gather this to plot series in ggplot:\ngfunction_long &lt;- gfunction_out %&gt;% \n  as.data.frame() %&gt;% \n  pivot_longer(cols = obs:hi, names_to = \"model\", values_to = \"g_val\")\n\n### Then make a graph in ggplot:\nggplot(data = gfunction_long, aes(x = r, y = g_val, group = model)) +\n  geom_line(aes(color = model)) +\n  theme_minimal() +\n  labs(x = 'radius (m)', y = 'G(r)')\n\n\n\n\n\nFigure 4: G Function Analysis of Oil Spills in CA. This figure conveys that our observations are very clustered because the observation line is above the theoretical line.\n\n\n\n\nCode\n### our observations are very clustered because it is above the theoretical line (CSR line in leture notes)\n\n\n\n\n\nA Snowy Plover is rehabilitated after being exposed to oil. Photo: Oiled Wildlife Care Network."
  },
  {
    "objectID": "posts/2024-02-10-spatial-data/index.html#works-cited",
    "href": "posts/2024-02-10-spatial-data/index.html#works-cited",
    "title": "Spatial Data Analysis",
    "section": "Works Cited",
    "text": "Works Cited\nOil Spill Incident Tracking [ds394]. California Department of Fish and Wildlife. Published July 29, 2009. Retrieved from: https://gis.data.ca.gov/datasets/CDFW::oil-spill-incident-tracking-ds394-1/about"
  },
  {
    "objectID": "posts/2024-12-17-benefits-transfer/index.html",
    "href": "posts/2024-12-17-benefits-transfer/index.html",
    "title": "Benefits Transfer",
    "section": "",
    "text": "Background\nCoastal wetlands provide a natural defense against storm surges and the effects of sea-level rise. They act like a sponge that dissipates wave impacts and reduces flood risk. Benefit transfer analysis can be used to determine the value of a 60 hectare salt marsh wetlands in Huntington Beach, California.\n\n\nData\nThe analysis will pull from these two studies:\n\nBayraktov et al. (2015): a meta-analysis of coastal wetland restoration costs. Specifically, you have access to the mangrove and salt marsh databases.\nCostanza et al. (2021): estimate the storm protection benefits (avoided damage).\n\nYou have part of the dataset of Bayraktov et al. (2015) and it has the following information:\n1) study_cluster: research belonging to a specific year\n2) wetland_type: mangrove or saltmarsh\n3) observation: research ID\n4) reference: authors\n5) ref_year: publication year\n6) country: country where the restoration project took place\n7) area_ha: restoration area\n8) total_cost_2010: total restoration cost in USD2010\n\n\nPart 1: Restoration costs\nYou will employ a benefit transfer to find the costs of restoring 60 hectares of wetland.\n\nInspect the dataset. What wetland type is most appropriate for your analysis? Filter the dataset for that wetland type.\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/madisoncalbert/Documents/BREN/Website/madicalbert.github.io\n\nrm(list = ls())\n\n# Load the data\ncost_df &lt;- read_csv(here(\"posts/2024-12-17-benefits-transfer/cost_df.csv\"))\n\nRows: 271 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): wetland_type, reference, country\ndbl (5): study_cluster, observation, ref_year, area_ha, total_cost_2010\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Filter for saltmarsh\nsaltmarsh_df &lt;- cost_df %&gt;%\n  filter(wetland_type == \"saltmarsh\") %&gt;% \n  drop_na()\n\nWe want saltmarsh wetlands for our analysis.\n\nCreate a new variable with the restoration costs per hectare. Then, choose one of the research studies and estimate the total costs of restoring 60 hectares in Huntington Beach. Explain your choice.\n\n\n# Create a new variable with the restoration costs per hectare\nsaltmarsh_df &lt;- saltmarsh_df %&gt;%\n  mutate(cost_per_ha = total_cost_2010 / area_ha)\n\n# Choose one of the research studies\nsaltmarsh_df %&gt;% \n  filter(country == \"USA\") %&gt;%\n  filter(area_ha &gt;= 60) \n\n# A tibble: 4 × 9\n  study_cluster wetland_type observation reference      ref_year country area_ha\n          &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1             1 saltmarsh              2 Adams CS, Ben…     1998 USA        160 \n2            18 saltmarsh             75 Milano G           1999 USA        122.\n3            26 saltmarsh            103 Society for E…     2007 USA        197.\n4            29 saltmarsh            106 Society for E…     2008 USA        130.\n# ℹ 2 more variables: total_cost_2010 &lt;dbl&gt;, cost_per_ha &lt;dbl&gt;\n\n# estimate the total costs of restoring 60 hectares in Huntington Beach based on observation #106\ntotal_cost_60_ha &lt;- saltmarsh_df %&gt;%\n  filter(observation == 106) %&gt;%\n  pull(cost_per_ha) * 60\ntotal_cost_60_ha\n\n[1] 5496802\n\n\nTo choose a research study, I filtered for projects in the USA with an area greater than or equal to 60 hectares. From these four results, I consulted with the larger data set from the Bayraktov et al. (2015) study to determine that observation #106 was a project in the San Francisco Bay area of California. Because our restoration site is in Huntington Beach, California, I chose observation #106 as the most appropriate for our analysis based on geographic location. Based on the restoration cost per hectare for observation #106, the total cost of restoring 60 hectares in Huntington Beach is $5,496,801.63\n\nWe are interested in the marginal cost of each additional hectare restored. Make a scatter plot of hectares on the y-axis and total restoration costs on the x-axis in the USA. Describe the relationship between total costs and wetland area restored.\n\n\n#filter for only USA\nsaltmarsh_usa &lt;- saltmarsh_df %&gt;% \n  filter(country==\"USA\")\n\n# Scatter plot of hectares on the y-axis and total restoration costs on the x-axis in the USA\nsaltmarsh_usa %&gt;%\n  ggplot(aes(x = total_cost_2010, y = area_ha)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_x_log10() +  # Log transform x-axis\n  scale_y_log10() +  # Log transform y-axis \n  labs(title = \"Total Restoration Costs by Area of Restored Saltmarsh\",\n       x = \"Total Restoration Costs (USD2010, log scale)\",\n       y = \"Wetland Area Restored (ha, log scale)\") + \n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nScatter plot of total restoration costs and hectares of saltmarsh wetland restored in the USA\n\n\n\n\nThe scatter plot shows a positive relationship between total restoration costs and hectares of wetland area restored. As the area of wetland restoration increases, the total restoration costs also increases. The relationship appears to be linear, with a few outliers that have higher restoration costs for a given area of wetland restoration.\n\nRun the following regression: total_cost_2010 = a + b*area_ha + error. Using the outcome of this regression, recalculate the total cost of restoring 60 hectares of wetland.\n\n\n# Run the regression\nlm_cost &lt;- lm(total_cost_2010 ~ area_ha, data = saltmarsh_usa)\nsummary(lm_cost)\n\n\nCall:\nlm(formula = total_cost_2010 ~ area_ha, data = saltmarsh_usa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-5239511  -541661  -352808    87971  7111292 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   446092     368986   1.209    0.234    \narea_ha        43861       7093   6.183 3.55e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2011000 on 37 degrees of freedom\nMultiple R-squared:  0.5082,    Adjusted R-squared:  0.4949 \nF-statistic: 38.23 on 1 and 37 DF,  p-value: 3.546e-07\n\n# Recalculate the total cost of restoring 60 hectares of wetland\ntotal_cost_60_ha_lm &lt;- coef(lm_cost)[1] + coef(lm_cost)[2] * 60\ntotal_cost_60_ha_lm\n\n(Intercept) \n    3077755 \n\n\nThe total cost from the regression is $3,077,755 to restore 60 hectares of wetland in Huntington Beach. This estimate is much lower than the previous estimate based on observation #106.\n\nDescribe one way you could improve your cost analysis and valuation.\n\nOne way to improve the cost analysis and valuation is to include additional variables that may influence the restoration costs. For example, factors such as the type of restoration activities, the condition of the wetland site, and the availability of resources could impact the total restoration costs. By including these variables in the analysis, we can better estimate the costs of restoring wetlands and provide more accurate benefit transfer estimates.\n\n\nPart 2: Storm protection benefits\nCostanza et al. (2021) analyzed 1288 coastal storms globally to calculate the storm protection benefits from wetlands. The authors obtained the following regression estimates:\n\\(ln(damages/GDP) = -7.992 - 0.236ln(wetlands) + 3.298ln(windspeed) - 0.55ln(speed) + 0.137(volume) - 0.058(time)\\)\nThey have the following variables:\n\ndamages/GDP,\nwind speed of the storm (windspeed),\nthe forward speed of the storm (speed),\nwetland area in the swath of the storm (wetlands),\nthe volume of water in the ocean proximal to the storm landfall (volume),\nand the year of the storm minus 1900 (time) as a (non-transformed) linear variable.\n\n\nInterpret the coefficient on ln(wetlands). (Hint: notice that the dependent variable is also log-transformed)\n\nThe coefficient on ln(wetlands) is -0.236. This coefficient indicates that a 1% increase in wetland area in the swath of the storm is associated with a 0.236% decrease in damages relative to GDP. Because both the dependent variable (damages/GDP) and the wetland area are log-transformed, this coefficient captures the percentage change in damages/GDP resulting from a 1% change in wetland area. In other words, a larger wetland area provides greater storm protection benefits by reducing the damages caused by coastal storms.\n\nCalculate the avoided damage of 60 additional hectares of wetlands in case of a storm like Hurricane Hilary. You have the following information: damages = $18 million, and the available wetland area today is 72 hectares. Assume the GDP doesn’t change (only damage moves), and all the remaining variables remain constant.\n\n\n#calculate change in wetland \ncurrent_wetland_area &lt;- 72\nadditional_wetland_area &lt;- 60\ntotal_wetland_area &lt;- current_wetland_area + additional_wetland_area \n# 72 + 60 = 132\n\n\nln_wetland &lt;- log(total_wetland_area/current_wetland_area)\n# ln_wetland\n#log(132/72) = 0.6061358034\n\n#reduction in damages\nln_damages &lt;- -0.236 * ln_wetland\n# - 0.236 * 0.6061358034 = -0.1430480496\n# ln_damages\n\n#calculate damage\ndamages &lt;- 18000000  \ndamage_new &lt;- damages * exp(ln_damages)\n#18,000,000 * exp(-0.1430480496)\n# damage_new\n\navoided_damage = damages - damage_new\n#18000000 - 15600824 = 2399176\navoided_damage \n\n[1] 2399176\n\n\nThe avoided damage from 60 additional hectares of wetlands in case of a storm like Hurricane Hilary is $2,399,176.\n\n\n\n\nCitationBibTeX citation:@online{calbert2024,\n  author = {Calbert, Madison},\n  title = {Benefits {Transfer}},\n  date = {2024-12-17},\n  url = {https://madicalbert.github.io/posts/2022-12-17-benefits-transfer/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCalbert, Madison. 2024. “Benefits Transfer.” December 17,\n2024. https://madicalbert.github.io/posts/2022-12-17-benefits-transfer/."
  },
  {
    "objectID": "posts/2024-03-03-nonlinear-least-squares/index.html",
    "href": "posts/2024-03-03-nonlinear-least-squares/index.html",
    "title": "Non-linear Least Squares",
    "section": "",
    "text": "Cultivated grain sorghum, El Campo, Texas, 2013. Photo by Lance Cheung, USDA (USDA on flickr, public domain).."
  },
  {
    "objectID": "posts/2024-03-03-nonlinear-least-squares/index.html#overview",
    "href": "posts/2024-03-03-nonlinear-least-squares/index.html#overview",
    "title": "Non-linear Least Squares",
    "section": "Overview",
    "text": "Overview\n\nPurpose\nFarmers need to understand the biology of plants and their responses to fertilizers to maximize yield. In this report, I conduct non-linear least squares on experimental growth data for three grains in Greece to make predictions on their yields. Because many crop and soil processes are better represented by nonlinear models comapred to linear models, nonlinear regression models are used to explore this data.\n\n\nData Source\n“We used data from Danalatos et al. (2009), which represent destructive measurements of aboveground biomass accumulation with time for three crops: fiber sorghum (F), sweet sorghum (S), and maize (M), growing in a deep fertile loamy soil of central Greece under two management practices: high and low input conditions, in 2008.” (Archontoulis 2015). The data used in this report was accessed by installing the “nlraa” package and then using library(nlraa).\nArchontoulis, S.V. and Miguez, F.E. (2015). Nonlinear Regression Models and Applications in Agricultural Research. Agronomy Journal, Volume 107, Issue 2. Retreived from: https://acsess.onlinelibrary.wiley.com/doi/10.2134/agronj2012.0506\n\n\nData Summary\nThe five variables in the dataset are Day of the Year (DOY), Block, Input, Crop, and Biomass yield in Mg/ha. The data variables are described as follows:\n\nYield = harvested biomass for three crops\nCrop = types of crop: maize (M), fiber sorghum (F) and sweet sorghum (S).\nInput = two levels of agronomic input, level 1 (Low) or 2 (High)\nBlock = four blocks in the experimental design (1, 2, 3, or 4)\nDOY = “day of year”"
  },
  {
    "objectID": "posts/2024-03-03-nonlinear-least-squares/index.html#pseudocode",
    "href": "posts/2024-03-03-nonlinear-least-squares/index.html#pseudocode",
    "title": "Non-linear Least Squares",
    "section": "PseudoCode",
    "text": "PseudoCode\n\nLoad libraries, load data, clean/tidy the data\nRun nls on one crop (Sorghum)\n\nModel Selection\nCreate R Function\nDefine initial Guess\nRun NLS\nEvaluate Results\n\nUse purrr to run NLS models for all 24 combinations of plot\nMake a “good looking” table\n\n\n\nCode\nlibrary(nlraa)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(Metrics)\nlibrary(cowplot)\nlibrary(nlme)\nlibrary(kableExtra)\nlibrary(purrr)\nlibrary(knitr)\nlibrary(patchwork)\n\n# glimpse(sm)\n\ndata &lt;- sm %&gt;% \n  clean_names()"
  },
  {
    "objectID": "posts/2024-03-03-nonlinear-least-squares/index.html#model-selection",
    "href": "posts/2024-03-03-nonlinear-least-squares/index.html#model-selection",
    "title": "Non-linear Least Squares",
    "section": "Model Selection",
    "text": "Model Selection\nI use the Beta function:\nY = Ymax (1 + (tc - t)/(tc - tm))(t/tc)^(tc / (tc - tm))\n“This model was selected because it captures the decline of biomass toward the end of the growing season and supplementary figure for the beta growth function. Also, the parameters have clear meaning and are very suitable to answer the research questions.” (Archontoulis 2015).\nThe parameters are defined as:\n\nY is the response variable (e.g., biomass)\nt is the explanatory variable (e.g., time),\nYasym or Ymax is the asymptotic or the maximum Y value, respectively,\ntm is the inflection point at which the growth rate is maximized,\nk controls the steepness of the curve,\nv deals with the asymmetric growth (if v = I, then Richards’ equation becomes logistic),\na and b are parameters that determine the shape of the curve,\nte is the time when Y = Yasym\ntc is the critical time for a switch-off to occur (eg., critical photoperiod),\nn is a parameter that determines the sharpness of the response\n\n\n\nCode\n### build a function\n\nbeta &lt;- function(doy, t_e, t_m, y_max){\n  out = y_max * (1 + (t_e - doy)/ (t_e - t_m)) * (doy/t_e)^(t_e/(t_e - t_m))\n  return(out)\n}\n\ny_max_guess &lt;- 20 \nt_e_guess &lt;- 240\nt_m_guess &lt;- 200\n\n\nguess &lt;- ggplot(data = data, aes(x = doy, y = yield, shape = crop)) + \n  geom_point() +\n  geom_smooth() +\n  facet_wrap(~input, labeller = labeller(input = c(\"2\" = \"High\", \"1\" = \"Low\"))) + \n  labs(x = \"Day of the Year\",\n       y = \" Dry biomass (Mg/ha)\",\n       shape = \"Crop\") +\n  theme_bw()"
  },
  {
    "objectID": "posts/2024-03-03-nonlinear-least-squares/index.html#one-crop-nls",
    "href": "posts/2024-03-03-nonlinear-least-squares/index.html#one-crop-nls",
    "title": "Non-linear Least Squares",
    "section": "One Crop NLS",
    "text": "One Crop NLS\nAfter defining the model, building a function, and making initial guesses, I now run the NLS on one crop: the high input sweet sorghum (S) crop. The selected parameter values, standard errors, and p-values of the estimated parameters are displayed in Table 1.\n\n\nCode\n### Sorghum Fields w/ High Inputs \n\nsm_high &lt;- data %&gt;% filter(crop == \"S\" & input == \"2\")\n\nsm_nls = nls(formula = yield ~ beta(doy, t_e, t_m, y_max), \n               data = sm_high, \n               start = list(t_e = t_e_guess, t_m = t_m_guess, y_max = y_max_guess), \n               trace = FALSE)\n\nsm_nls %&gt;%\n  broom::tidy() %&gt;%\n  mutate(p.value = ifelse(p.value &lt;= 0.05, \"&lt;0.05\")) %&gt;%\n  kbl(digits = 2, align = NULL) %&gt;%\n  kable_classic() \n\n\n\n\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\n\n\n\n\nt_e\n\n\n281.59\n\n\n2.08\n\n\n135.33\n\n\n&lt;0.05\n\n\n\n\nt_m\n\n\n244.78\n\n\n3.51\n\n\n69.70\n\n\n&lt;0.05\n\n\n\n\ny_max\n\n\n39.82\n\n\n2.25\n\n\n17.71\n\n\n&lt;0.05\n\n\n\n\nTable 1: The selected parameter values, standard errors, and p-values of the estimated parameters for NLS on the high input sweet sorghum (S) crop.\n\n\n\n\nCode\nsm_p2 &lt;- sm_high %&gt;%\n  mutate(predict = predict(sm_nls, newdata=.))\n\nggplot(sm_p2, aes(x = doy, y = yield)) +\n  geom_point() +\n  geom_line(aes(x = doy, y = predict), linewidth = 1, color = \"red\")+ \n  labs(x = \"Day of the Year\",\n       y = \" Dry biomass (Mg/ha)\",\n       color = \"Crop\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nFigure 1: The fitted model on top of the the high input sweet sorghum (S) crop data."
  },
  {
    "objectID": "posts/2024-03-03-nonlinear-least-squares/index.html#nls-for-all-using-purrr",
    "href": "posts/2024-03-03-nonlinear-least-squares/index.html#nls-for-all-using-purrr",
    "title": "Non-linear Least Squares",
    "section": "NLS for All (using purrr)",
    "text": "NLS for All (using purrr)\nNow I run the NLS models for all 24 combinations of plot, input level, and crop type using purrr. Table 2 portrays the RMSE and chosen parameter values of the best fitted models for each species.\n\n\nCode\n### define a function for NLS for all\n\ncrop &lt;- function(nls_test){\n  nls(yield ~ beta(doy, t_e, t_m, y_max), \n  data = nls_test, \n  start = list(t_e = t_e_guess, t_m = t_m_guess, y_max = y_max_guess))\n}\n\n### purrr\n\nyield_all &lt;- data %&gt;%\n  group_by(block, input, crop) %&gt;%\n  nest() %&gt;%\n  mutate(nls_model = map(data,~crop(.x))) %&gt;%\n  mutate(predictions = map2(nls_model, data, ~predict(.x, newdata = .y))) %&gt;%\n  mutate(rmse = map2_dbl(predictions, data, ~ Metrics::rmse(.x, .y$yield))) %&gt;%\n  mutate(smooth = map(nls_model, ~predict(.x, newdata = list(doy = seq(147, 306)))))\n\n\n\n\nCode\nrmse_table &lt;- yield_all %&gt;% \n  group_by(crop) %&gt;% \n  summarise(rmse = min(rmse))\n\nlow_rmse &lt;- yield_all %&gt;% \n  filter(rmse %in% rmse_table$rmse)\n\nlow_rmse_M &lt;- broom::tidy(low_rmse$nls_model[[1]]) %&gt;% \n  mutate(crop = \"Maize(M)\")\n\n\nlow_rmse_S &lt;- broom::tidy(low_rmse$nls_model[[2]]) %&gt;% \n  mutate(crop = \"Sweet Sorghum (S))\")\n\n\nlow_rmse_F &lt;- broom::tidy(low_rmse$nls_model[[3]]) %&gt;% \n  mutate(crop = \"Fiber Sorgum (F)))\")\n\nlow_rmse_combined &lt;- bind_rows(low_rmse_M, low_rmse_S, low_rmse_F)\n\nlow_rmse_combined &lt;- low_rmse_combined[, c(\"crop\", setdiff(names(low_rmse_combined), \"crop\"))]\n\nlow_rmse_combined %&gt;%\n  kbl(digits = 2, align = NULL) %&gt;%\n  kable_classic() \n\n\n\n\n\n\ncrop\n\n\nterm\n\n\nestimate\n\n\nstd.error\n\n\nstatistic\n\n\np.value\n\n\n\n\n\n\nMaize(M)\n\n\nt_e\n\n\n252.57\n\n\n1.87\n\n\n135.13\n\n\n0\n\n\n\n\nMaize(M)\n\n\nt_m\n\n\n225.23\n\n\n1.93\n\n\n116.92\n\n\n0\n\n\n\n\nMaize(M)\n\n\ny_max\n\n\n18.93\n\n\n0.84\n\n\n22.55\n\n\n0\n\n\n\n\nSweet Sorghum (S))\n\n\nt_e\n\n\n278.35\n\n\n1.82\n\n\n153.18\n\n\n0\n\n\n\n\nSweet Sorghum (S))\n\n\nt_m\n\n\n245.77\n\n\n3.82\n\n\n64.42\n\n\n0\n\n\n\n\nSweet Sorghum (S))\n\n\ny_max\n\n\n31.35\n\n\n2.05\n\n\n15.31\n\n\n0\n\n\n\n\nFiber Sorgum (F)))\n\n\nt_e\n\n\n280.43\n\n\n1.84\n\n\n152.49\n\n\n0\n\n\n\n\nFiber Sorgum (F)))\n\n\nt_m\n\n\n245.00\n\n\n3.54\n\n\n69.24\n\n\n0\n\n\n\n\nFiber Sorgum (F)))\n\n\ny_max\n\n\n29.06\n\n\n1.66\n\n\n17.53\n\n\n0\n\n\n\n\nTable 2: The RMSE and chosen parameter values of the best fitted models for each species – fiber sorghum (F), sweet sorghum (S), and maize (M).\n\n\n\n\nCode\n# Unnest predictions from data and clean maize data\nun_df &lt;- yield_all %&gt;% \n  filter(block==1) %&gt;% \n  tidyr::unnest(smooth) %&gt;% \n  mutate(doy=seq(147,306)) %&gt;% \n  filter(!(doy&gt;263 & crop==\"M\"))\n\n# Create a dataframe to add corn data\nhi_filter &lt;- data %&gt;% \n  filter(block == 1 & input == 2)\n\nlow_filter &lt;- data %&gt;% \n  filter(block == 1 & input == 1)\n\n\n\n\nCode\n# Make graphs\nhi_plot &lt;- un_df %&gt;%\n  filter(block == 1 & input == 2) %&gt;%\n  ggplot() +\n  geom_point(data = hi_filter, aes(x = doy, y = yield, shape = crop)) +\n  geom_line(aes(x = doy, y = smooth, linetype = crop)) +labs(y = \" \", x = \"DOY\", color = \" \")+\n  theme_minimal()\n\n# hi_plot\n\n\nlow_plot&lt;-un_df |&gt; \n  filter(block==1 & input==1) |&gt; \n  ggplot()+\n  geom_point(data=low_filter,aes(x=doy,y=yield,shape=crop))+\n  geom_line(aes(x=doy,y=smooth,linetype=crop))+\n  labs(y = \"Biomass Yield\", x = \"DOY\", color = \"\")+\n  theme_minimal()\n\n# low_plot\n\n\ncombined_plot &lt;- low_plot + hi_plot\n\ncombined_plot + plot_layout(guides = \"collect\") + plot_annotation(tag_levels = \"A\")\n\n\n\n\n\nFigure 2: Observed data and fit for the final model for three crops: maize (M), fiber sorghum (F), and sweet sorghum (S) within Block 1. Plot A is the low input crops and Plot B is the high input crops."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Madison Calbert",
    "section": "",
    "text": "Hello :-) I am a conservation ecologist who loves plants and wildlife."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Madison Calbert",
    "section": "",
    "text": "Hello :-) I am a conservation ecologist who loves plants and wildlife."
  },
  {
    "objectID": "index.html#professional-interests",
    "href": "index.html#professional-interests",
    "title": "Madison Calbert",
    "section": "Professional Interests",
    "text": "Professional Interests\nWildlife Conservation | Restoration Ecology | Endangered Species Management & Recovery | Landscape Ecology | Habitat Connectivity"
  },
  {
    "objectID": "index.html#skillset",
    "href": "index.html#skillset",
    "title": "Madison Calbert",
    "section": "Skillset",
    "text": "Skillset\nWildlife & Plant ID | Data Science | R Programming Language | Geospatial Analysis | Excel | Project Management | Communication | Stakeholder Engagement"
  },
  {
    "objectID": "index.html#recent-ongoing-projects",
    "href": "index.html#recent-ongoing-projects",
    "title": "Madison Calbert",
    "section": "Recent & Ongoing Projects",
    "text": "Recent & Ongoing Projects\nEvaluating Ecological Conservation Gaps Across a Proposed Sentinel Landscape (Connectivity Analysis)\nGroundwater in Southern California"
  },
  {
    "objectID": "wildlife.html",
    "href": "wildlife.html",
    "title": "Wildlife",
    "section": "",
    "text": "Santa Cruz Island fox (Santa Cruz Island), handled under the supervision of permitted biologist.\n\n\n\n\n\nCalifornia tiger salamander (Concord, CA), handled under the supervision of permitted biologist.\n\n\n\n\n\nJuvenile Gilbert’s skink (Concord, CA)"
  }
]